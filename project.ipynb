{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "85e65dd126a5708a4bfedf05126b2dd2366f42efdc5cb039c58cde41fbf2f274"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### _This is the topic modelling Python program of the workgroup No.4, for the final project of the Digital Humanities Lab course at the Universiteit van Amsterdam."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universiteit van Amsterdam\n",
    "# Digital Humanities Lab, WG04\n",
    "# Final project - Topic modelling program\n",
    "# project.ipynb"
   ]
  },
  {
   "source": [
    "### _Importing all the necessary tools and libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Miller\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os, fitz, string, nltk, PyPDF2, gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from functions import readPDF, processPDF, lemmatizeAndStem, toDataFrame, listOfWords, wordsPerArticle, tfidfCorpus"
   ]
  },
  {
   "source": [
    "### _Reading all the PDF files that can be found in the given folder, preprocessing, tokenizing, lemmatizing, stemming, and then turning into a DataFrame for further usage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the files and convert from pdf to string/dict\n",
    "articles = readPDF('open access articles')\n",
    "\n",
    "# Basic text-processing, tokenize and filtering\n",
    "processedArticles = processPDF(articles)\n",
    "\n",
    "# Gensim, LDA stuff\n",
    "dataFrame = toDataFrame(processedArticles)"
   ]
  },
  {
   "source": [
    "### _A dataframe containing the pdf file names, and all of its contents, already processed. \n",
    "#### _For further information how they are processed, please check the functions.py file and the processPDF function or use \"pydoc functions.functions\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                            file_name  \\\n0                   1-s2.0-S0550321316301341-main.pdf   \n1               43-Article Text-125-1-10-20161227.pdf   \n2                                             8.8.pdf   \n3           8037-Article Text-26428-1-10-20130627.pdf   \n4                            Acta_acu-201102-0003.pdf   \n5                             applsci-10-05589-v3.pdf   \n6   Cao2018_Article_StudyOnPM25PollutionAndTheMort...   \n7                               energies-08-03882.pdf   \n8                                  fpsyg-04-00479.pdf   \n9                                 ijerph-13-01268.pdf   \n10                          NB_article_66276_en_1.pdf   \n11                                            PDF.pdf   \n12                             religions-09-00361.pdf   \n13                              s12906-017-1783-3.pdf   \n14  Wang2018_Article_ARiskEvaluationModelForChanne...   \n\n                                              content  \n0   [avail, onlin, www.sciencedirect.com, scienced...  \n1   [review, critic, view, polem, rudolf, siebert,...  \n2   [compar, analysi, methodolog, islam, jurist, u...  \n3   [mediekultur, journal, media, communic, resear...  \n4   [acta, universitati, agricultura, silvicultura...  \n5   [appli, scienc, articl, space, alloc, method, ...  \n6   [research, articl, open, access, studi, pm2.5,...  \n7   [energi, 2015, 3882-3902, doi:10.3390/en805388...  \n8   [origin, research, articl, publish, juli, 2013...  \n9   [intern, journal, environment, research, publi...  \n10  [classic, biolog, control, insect, pest, europ...  \n11  [origin, paper, focus, review, smartphon, diet...  \n12  [religion, articl, religion, fals, religion, e...  \n13  [meet, abstract, open, access, world, congress...  \n14  [research, open, access, risk, evalu, model, c...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1-s2.0-S0550321316301341-main.pdf</td>\n      <td>[avail, onlin, www.sciencedirect.com, scienced...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43-Article Text-125-1-10-20161227.pdf</td>\n      <td>[review, critic, view, polem, rudolf, siebert,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.8.pdf</td>\n      <td>[compar, analysi, methodolog, islam, jurist, u...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8037-Article Text-26428-1-10-20130627.pdf</td>\n      <td>[mediekultur, journal, media, communic, resear...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Acta_acu-201102-0003.pdf</td>\n      <td>[acta, universitati, agricultura, silvicultura...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>applsci-10-05589-v3.pdf</td>\n      <td>[appli, scienc, articl, space, alloc, method, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Cao2018_Article_StudyOnPM25PollutionAndTheMort...</td>\n      <td>[research, articl, open, access, studi, pm2.5,...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>energies-08-03882.pdf</td>\n      <td>[energi, 2015, 3882-3902, doi:10.3390/en805388...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fpsyg-04-00479.pdf</td>\n      <td>[origin, research, articl, publish, juli, 2013...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ijerph-13-01268.pdf</td>\n      <td>[intern, journal, environment, research, publi...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NB_article_66276_en_1.pdf</td>\n      <td>[classic, biolog, control, insect, pest, europ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>PDF.pdf</td>\n      <td>[origin, paper, focus, review, smartphon, diet...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>religions-09-00361.pdf</td>\n      <td>[religion, articl, religion, fals, religion, e...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>s12906-017-1783-3.pdf</td>\n      <td>[meet, abstract, open, access, world, congress...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Wang2018_Article_ARiskEvaluationModelForChanne...</td>\n      <td>[research, open, access, risk, evalu, model, c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = listOfWords(dataFrame)\n",
    "\n",
    "wordsPerArticle = wordsPerArticle(dataFrame)\n",
    "\n",
    "tfidfCorpus = tfidfCorpus(listOfWords, dataFrame)"
   ]
  },
  {
   "source": [
    "### _Generating a random number of topics with a simple LDA model, using the list of words per articles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \nWords: 0.001*\"studi\" + 0.001*\"control\" + 0.000*\"group\" + 0.000*\"religion\" + 0.000*\"medicin\" + 0.000*\"field\" + 0.000*\"patient\" + 0.000*\"result\" + 0.000*\"trial\" + 0.000*\"target\"\nTopic: 1 \nWords: 0.001*\"group\" + 0.001*\"studi\" + 0.001*\"medicin\" + 0.000*\"patient\" + 0.000*\"land\" + 0.000*\"control\" + 0.000*\"result\" + 0.000*\"trial\" + 0.000*\"method\" + 0.000*\"effect\"\nTopic: 2 \nWords: 0.014*\"health\" + 0.010*\"pm2.5\" + 0.010*\"studi\" + 0.010*\"china\" + 0.008*\"cancer\" + 0.008*\"theori\" + 0.008*\"lung\" + 0.006*\"servic\" + 0.006*\"mortal\" + 0.006*\"field\"\nTopic: 3 \nWords: 0.000*\"medicin\" + 0.000*\"patient\" + 0.000*\"chang\" + 0.000*\"control\" + 0.000*\"studi\" + 0.000*\"land\" + 0.000*\"model\" + 0.000*\"china\" + 0.000*\"cancer\" + 0.000*\"method\"\nTopic: 4 \nWords: 0.001*\"medicin\" + 0.001*\"studi\" + 0.001*\"patient\" + 0.001*\"result\" + 0.001*\"health\" + 0.001*\"effect\" + 0.001*\"group\" + 0.000*\"complementari\" + 0.000*\"field\" + 0.000*\"control\"\nTopic: 5 \nWords: 0.001*\"studi\" + 0.001*\"patient\" + 0.001*\"medicin\" + 0.000*\"method\" + 0.000*\"evalu\" + 0.000*\"health\" + 0.000*\"group\" + 0.000*\"result\" + 0.000*\"effect\" + 0.000*\"2017\"\nTopic: 6 \nWords: 0.007*\"عم��ا\" + 0.005*\"لوسر\" + 0.005*\"jurist\" + 0.004*\"حيج��لاو\" + 0.004*\"contradict\" + 0.004*\"conflict\" + 0.003*\"ام�دحأ\" + 0.003*\"ن�ضراعتﳌا\" + 0.003*\"نو�ي\" + 0.003*\"اذ�و\"\nTopic: 7 \nWords: 0.018*\"medicin\" + 0.014*\"patient\" + 0.009*\"studi\" + 0.008*\"complementari\" + 0.008*\"treatment\" + 0.007*\"group\" + 0.007*\"altern\" + 0.007*\"univers\" + 0.007*\"result\" + 0.007*\"2017\"\nTopic: 8 \nWords: 0.001*\"medicin\" + 0.001*\"studi\" + 0.000*\"group\" + 0.000*\"patient\" + 0.000*\"religion\" + 0.000*\"result\" + 0.000*\"health\" + 0.000*\"complementari\" + 0.000*\"control\" + 0.000*\"2017\"\nTopic: 9 \nWords: 0.001*\"studi\" + 0.000*\"patient\" + 0.000*\"medicin\" + 0.000*\"result\" + 0.000*\"group\" + 0.000*\"health\" + 0.000*\"univers\" + 0.000*\"model\" + 0.000*\"control\" + 0.000*\"chang\"\nTopic: 10 \nWords: 0.028*\"land\" + 0.019*\"evalu\" + 0.016*\"chang\" + 0.013*\"model\" + 0.012*\"index\" + 0.011*\"channel\" + 0.009*\"navig\" + 0.009*\"weight\" + 0.008*\"map\" + 0.007*\"cover\"\nTopic: 11 \nWords: 0.001*\"studi\" + 0.000*\"medicin\" + 0.000*\"patient\" + 0.000*\"health\" + 0.000*\"group\" + 0.000*\"control\" + 0.000*\"evalu\" + 0.000*\"complementari\" + 0.000*\"altern\" + 0.000*\"result\"\nTopic: 12 \nWords: 0.031*\"field\" + 0.030*\"forc\" + 0.024*\"trial\" + 0.019*\"group\" + 0.019*\"catch\" + 0.018*\"learn\" + 0.016*\"consolid\" + 0.014*\"motor\" + 0.013*\"perform\" + 0.011*\"interfer\"\nTopic: 13 \nWords: 0.028*\"religion\" + 0.022*\"media\" + 0.014*\"mediat\" + 0.008*\"nordic\" + 0.008*\"book\" + 0.007*\"studi\" + 0.006*\"field\" + 0.006*\"theori\" + 0.006*\"cultur\" + 0.006*\"perspect\"\nTopic: 14 \nWords: 0.001*\"vehicl\" + 0.001*\"control\" + 0.001*\"space\" + 0.001*\"model\" + 0.001*\"platoon\" + 0.001*\"follow\" + 0.001*\"result\" + 0.001*\"valu\" + 0.001*\"studi\" + 0.001*\"simul\"\nTopic: 15 \nWords: 0.001*\"app\" + 0.001*\"studi\" + 0.001*\"health\" + 0.001*\"patient\" + 0.000*\"medicin\" + 0.000*\"2017\" + 0.000*\"group\" + 0.000*\"featur\" + 0.000*\"behavior\" + 0.000*\"food\"\nTopic: 16 \nWords: 0.031*\"control\" + 0.030*\"target\" + 0.029*\"agent\" + 0.019*\"introduct\" + 0.019*\"success\" + 0.019*\"biolog\" + 0.013*\"pest\" + 0.010*\"plant\" + 0.010*\"number\" + 0.010*\"impact\"\nTopic: 17 \nWords: 0.000*\"medicin\" + 0.000*\"studi\" + 0.000*\"patient\" + 0.000*\"group\" + 0.000*\"result\" + 0.000*\"health\" + 0.000*\"evalu\" + 0.000*\"theori\" + 0.000*\"effect\" + 0.000*\"model\"\nTopic: 18 \nWords: 0.035*\"vehicl\" + 0.026*\"space\" + 0.023*\"platoon\" + 0.021*\"polici\" + 0.015*\"control\" + 0.013*\"valu\" + 0.012*\"game\" + 0.012*\"veloc\" + 0.011*\"follow\" + 0.011*\"error\"\nTopic: 19 \nWords: 0.000*\"studi\" + 0.000*\"medicin\" + 0.000*\"religion\" + 0.000*\"control\" + 0.000*\"health\" + 0.000*\"space\" + 0.000*\"vehicl\" + 0.000*\"result\" + 0.000*\"patient\" + 0.000*\"treatment\"\nTopic: 20 \nWords: 0.001*\"studi\" + 0.001*\"medicin\" + 0.001*\"patient\" + 0.000*\"health\" + 0.000*\"group\" + 0.000*\"field\" + 0.000*\"complementari\" + 0.000*\"2017\" + 0.000*\"trial\" + 0.000*\"effect\"\nTopic: 21 \nWords: 0.033*\"religion\" + 0.017*\"religi\" + 0.011*\"school\" + 0.010*\"educ\" + 0.009*\"teacher\" + 0.008*\"danger\" + 0.007*\"respons\" + 0.007*\"essenti\" + 0.007*\"question\" + 0.006*\"posit\"\nTopic: 22 \nWords: 0.001*\"medicin\" + 0.001*\"control\" + 0.001*\"patient\" + 0.001*\"altern\" + 0.001*\"studi\" + 0.000*\"result\" + 0.000*\"group\" + 0.000*\"univers\" + 0.000*\"health\" + 0.000*\"2017\"\nTopic: 23 \nWords: 0.023*\"app\" + 0.011*\"food\" + 0.010*\"featur\" + 0.009*\"usabl\" + 0.007*\"diet\" + 0.007*\"user\" + 0.007*\"mhealth\" + 0.007*\"behavior\" + 0.007*\"studi\" + 0.007*\"chang\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(wordsPerArticle, num_topics=random.randrange(10,26), id2word=listOfWords, passes=random.randrange(5,11), workers=3)\n",
    "\n",
    "for index, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, topic))"
   ]
  },
  {
   "source": [
    "### _Generating a random number of topics with a TF-IDF LDA model, using the TF-IDF model's corpus of words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 1 Word: 0.001*\"medicin\" + 0.001*\"complementari\" + 0.001*\"patient\" + 0.000*\"therapi\" + 0.000*\"suppl\" + 0.000*\"treatment\" + 0.000*\"acupunctur\" + 0.000*\"altern\" + 0.000*\"clinic\" + 0.000*\"pain\"\nTopic: 2 Word: 0.001*\"religion\" + 0.001*\"religi\" + 0.000*\"teacher\" + 0.000*\"danger\" + 0.000*\"teach\" + 0.000*\"respond\" + 0.000*\"essenti\" + 0.000*\"educ\" + 0.000*\"question\" + 0.000*\"2018\"\nTopic: 3 Word: 0.001*\"land\" + 0.000*\"grassland\" + 0.000*\"map\" + 0.000*\"1932\" + 0.000*\"spatial\" + 0.000*\"topograph\" + 0.000*\"1954\" + 0.000*\"reconstruct\" + 0.000*\"settlement\" + 0.000*\"unus\"\nTopic: 4 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 5 Word: 0.001*\"pm2.5\" + 0.001*\"health\" + 0.001*\"cancer\" + 0.001*\"lung\" + 0.001*\"agent\" + 0.001*\"biolog\" + 0.001*\"pest\" + 0.000*\"hospit\" + 0.000*\"household\" + 0.000*\"chronic\"\nTopic: 6 Word: 0.001*\"platoon\" + 0.001*\"vehicl\" + 0.000*\"veloc\" + 0.000*\"stabil\" + 0.000*\"game\" + 0.000*\"polici\" + 0.000*\"traffic\" + 0.000*\"space\" + 0.000*\"cooper\" + 0.000*\"flow\"\nTopic: 7 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 8 Word: 0.001*\"liouvill\" + 0.001*\"fusion\" + 0.000*\"sector\" + 0.000*\"super\" + 0.000*\"sarkissian\" + 0.000*\"poghosyan\" + 0.000*\"matrix\" + 0.000*\"conform\" + 0.000*\"458–479\" + 0.000*\"defect\"\nTopic: 9 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 10 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 11 Word: 0.001*\"religion\" + 0.000*\"mediat\" + 0.000*\"media\" + 0.000*\"nordic\" + 0.000*\"hjarvard\" + 0.000*\"lövheim\" + 0.000*\"book\" + 0.000*\"religi\" + 0.000*\"laura\" + 0.000*\"feldt\"\nTopic: 12 Word: 0.001*\"manifesto\" + 0.001*\"religion\" + 0.000*\"religi\" + 0.000*\"wholli\" + 0.000*\"dialect\" + 0.000*\"antagon\" + 0.000*\"social\" + 0.000*\"1/2015\" + 0.000*\"societi\" + 0.000*\"hopeless\"\nTopic: 13 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 14 Word: 0.001*\"app\" + 0.001*\"busi\" + 0.001*\"catch\" + 0.001*\"combat\" + 0.001*\"consolid\" + 0.000*\"competit\" + 0.000*\"usabl\" + 0.000*\"forc\" + 0.000*\"trial\" + 0.000*\"bart\"\nTopic: 15 Word: 0.000*\"medicin\" + 0.000*\"complementari\" + 0.000*\"patient\" + 0.000*\"acupunctur\" + 0.000*\"treatment\" + 0.000*\"altern\" + 0.000*\"therapi\" + 0.000*\"clinic\" + 0.000*\"suppl\" + 0.000*\"herbal\"\nTopic: 16 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 17 Word: 0.000*\"عم��ا\" + 0.000*\"لوسر\" + 0.000*\"jurist\" + 0.000*\"حيج��لاو\" + 0.000*\"نو�ي\" + 0.000*\"بتكلا\" + 0.000*\"ن�ضراعتﳌا\" + 0.000*\"ام�دحأ\" + 0.000*\"اذ�و\" + 0.000*\"contradict\"\nTopic: 18 Word: 0.001*\"navig\" + 0.001*\"channel\" + 0.001*\"gray\" + 0.000*\"index\" + 0.000*\"gray-fuzzi\" + 0.000*\"first-level\" + 0.000*\"weight\" + 0.000*\"evalu\" + 0.000*\"set-valu\" + 0.000*\"safeti\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidfCorpus, num_topics=random.randrange(10,26), id2word=listOfWords, passes=random.randrange(5,11), workers=3)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "source": [
    "### _Evaluating the performance and accuracy of the simple LDA model, with a randomly choosen article's index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The random generated article's index:  2\n\nScore: 0.8808422684669495\t \nTopic: 0.007*\"عم��ا\" + 0.005*\"لوسر\" + 0.005*\"jurist\" + 0.004*\"حيج��لاو\" + 0.004*\"contradict\" + 0.004*\"conflict\" + 0.003*\"ام�دحأ\" + 0.003*\"ن�ضراعتﳌا\" + 0.003*\"نو�ي\" + 0.003*\"اذ�و\"\n\nScore: 0.11812471598386765\t \nTopic: 0.028*\"religion\" + 0.022*\"media\" + 0.014*\"mediat\" + 0.008*\"nordic\" + 0.008*\"book\" + 0.007*\"studi\" + 0.006*\"field\" + 0.006*\"theori\" + 0.006*\"cultur\" + 0.006*\"perspect\"\n"
     ]
    }
   ],
   "source": [
    "randomArticle = random.randrange(0,15)\n",
    "print(\"The random generated article's index: \", randomArticle)\n",
    "\n",
    "for index, score in sorted(lda_model[wordsPerArticle[randomArticle]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "source": [
    "### _Evaluating the performance and accuracy of the TF-IDF LDA model, with a randomly choosen article's index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The random generated article's index:  7\n\nScore: 0.9302291870117188\t \nTopic: 0.001*\"land\" + 0.000*\"grassland\" + 0.000*\"map\" + 0.000*\"1932\" + 0.000*\"spatial\" + 0.000*\"topograph\" + 0.000*\"1954\" + 0.000*\"reconstruct\" + 0.000*\"settlement\" + 0.000*\"unus\"\n\nScore: 0.055066995322704315\t \nTopic: 0.001*\"pm2.5\" + 0.001*\"health\" + 0.001*\"cancer\" + 0.001*\"lung\" + 0.001*\"agent\" + 0.001*\"biolog\" + 0.001*\"pest\" + 0.000*\"hospit\" + 0.000*\"household\" + 0.000*\"chronic\"\n\nScore: 0.010687386617064476\t \nTopic: 0.001*\"navig\" + 0.001*\"channel\" + 0.001*\"gray\" + 0.000*\"index\" + 0.000*\"gray-fuzzi\" + 0.000*\"first-level\" + 0.000*\"weight\" + 0.000*\"evalu\" + 0.000*\"set-valu\" + 0.000*\"safeti\"\n"
     ]
    }
   ],
   "source": [
    "randomArticle_tfidf = random.randrange(0,15)\n",
    "print(\"The random generated article's index: \", randomArticle_tfidf)\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[wordsPerArticle[randomArticle_tfidf]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "source": [
    "## _Acknowledgements and citations\n",
    "\n",
    "#### _To build our topic modelling tool for the final project of this course, Sandra Li's article, and GitHub repository was a great help, and we implemented some of her ideas and methods, - such as the lemmatizing and stemming of words - to our program, and followed the pipeline of her program.\n",
    "#### _The article: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "#### _The repository: https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb\n",
    "\n",
    "#### _Any other part of the project is coded by ourselves, and we only used the documentations of the imported modules.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### _Universiteit van Amsterdam\n",
    "### _Digital Humanities Lab - WG04\n",
    "### _Final project\n",
    "\n",
    "#### _This Python program for the project was coded entirely by Tamás Molnár\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}