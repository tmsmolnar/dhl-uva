{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "85e65dd126a5708a4bfedf05126b2dd2366f42efdc5cb039c58cde41fbf2f274"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Miller\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Miller\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os, fitz, string, nltk, PyPDF2, gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from functions import readPDF, processPDF, lemmatizeAndStem, toDataFrame, listOfWords, corpusOfWords, tfidfCorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the files and convert from pdf to string/dict\n",
    "articles = readPDF('open access articles')\n",
    "\n",
    "# Basic text-processing, tokenize and filtering\n",
    "processedArticles = processPDF(articles)\n",
    "\n",
    "# Gensim, LDA stuff\n",
    "dataFrame = toDataFrame(processedArticles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                            file_name  \\\n0                   1-s2.0-S0550321316301341-main.pdf   \n1               43-Article Text-125-1-10-20161227.pdf   \n2                                             8.8.pdf   \n3           8037-Article Text-26428-1-10-20130627.pdf   \n4                            Acta_acu-201102-0003.pdf   \n5                             applsci-10-05589-v3.pdf   \n6   Cao2018_Article_StudyOnPM25PollutionAndTheMort...   \n7                               energies-08-03882.pdf   \n8                                  fpsyg-04-00479.pdf   \n9                                 ijerph-13-01268.pdf   \n10                          NB_article_66276_en_1.pdf   \n11                                            PDF.pdf   \n12                             religions-09-00361.pdf   \n13                              s12906-017-1783-3.pdf   \n14  Wang2018_Article_ARiskEvaluationModelForChanne...   \n\n                                              content  \n0   [avail, onlin, www.sciencedirect.com, scienced...  \n1   [review, critic, view, polem, rudolf, siebert,...  \n2   [compar, analysi, methodolog, islam, jurist, u...  \n3   [mediekultur, journal, media, communic, resear...  \n4   [acta, universitati, agricultura, silvicultura...  \n5   [appli, scienc, articl, space, alloc, method, ...  \n6   [research, articl, open, access, studi, pm2.5,...  \n7   [energi, 2015, 3882-3902, doi:10.3390/en805388...  \n8   [origin, research, articl, publish, juli, 2013...  \n9   [intern, journal, environment, research, publi...  \n10  [classic, biolog, control, insect, pest, europ...  \n11  [origin, paper, focus, review, smartphon, diet...  \n12  [religion, articl, religion, fals, religion, e...  \n13  [meet, abstract, open, access, world, congress...  \n14  [research, open, access, risk, evalu, model, c...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1-s2.0-S0550321316301341-main.pdf</td>\n      <td>[avail, onlin, www.sciencedirect.com, scienced...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43-Article Text-125-1-10-20161227.pdf</td>\n      <td>[review, critic, view, polem, rudolf, siebert,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.8.pdf</td>\n      <td>[compar, analysi, methodolog, islam, jurist, u...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8037-Article Text-26428-1-10-20130627.pdf</td>\n      <td>[mediekultur, journal, media, communic, resear...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Acta_acu-201102-0003.pdf</td>\n      <td>[acta, universitati, agricultura, silvicultura...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>applsci-10-05589-v3.pdf</td>\n      <td>[appli, scienc, articl, space, alloc, method, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Cao2018_Article_StudyOnPM25PollutionAndTheMort...</td>\n      <td>[research, articl, open, access, studi, pm2.5,...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>energies-08-03882.pdf</td>\n      <td>[energi, 2015, 3882-3902, doi:10.3390/en805388...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fpsyg-04-00479.pdf</td>\n      <td>[origin, research, articl, publish, juli, 2013...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ijerph-13-01268.pdf</td>\n      <td>[intern, journal, environment, research, publi...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NB_article_66276_en_1.pdf</td>\n      <td>[classic, biolog, control, insect, pest, europ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>PDF.pdf</td>\n      <td>[origin, paper, focus, review, smartphon, diet...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>religions-09-00361.pdf</td>\n      <td>[religion, articl, religion, fals, religion, e...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>s12906-017-1783-3.pdf</td>\n      <td>[meet, abstract, open, access, world, congress...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Wang2018_Article_ARiskEvaluationModelForChanne...</td>\n      <td>[research, open, access, risk, evalu, model, c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = listOfWords(dataFrame)\n",
    "\n",
    "corpusOfWords = corpusOfWords(dataFrame)\n",
    "\n",
    "tfidfCorpus = tfidfCorpus(listOfWords, dataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 Word: 0.001*\"religion\" + 0.001*\"religi\" + 0.000*\"teacher\" + 0.000*\"danger\" + 0.000*\"teach\" + 0.000*\"respond\" + 0.000*\"essenti\" + 0.000*\"educ\" + 0.000*\"question\" + 0.000*\"2018\"\nTopic: 1 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 2 Word: 0.001*\"pm2.5\" + 0.001*\"lung\" + 0.000*\"cancer\" + 0.000*\"pollut\" + 0.000*\"mortal\" + 0.000*\"spatial\" + 0.000*\"china\" + 0.000*\"provinc\" + 0.000*\"regress\" + 0.000*\"health\"\nTopic: 3 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 4 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 5 Word: 0.001*\"land\" + 0.001*\"medicin\" + 0.001*\"catch\" + 0.001*\"trial\" + 0.001*\"complementari\" + 0.001*\"consolid\" + 0.001*\"patient\" + 0.000*\"forc\" + 0.000*\"shadmehr\" + 0.000*\"motor\"\nTopic: 6 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 7 Word: 0.001*\"platoon\" + 0.001*\"vehicl\" + 0.000*\"veloc\" + 0.000*\"stabil\" + 0.000*\"game\" + 0.000*\"polici\" + 0.000*\"traffic\" + 0.000*\"space\" + 0.000*\"cooper\" + 0.000*\"flow\"\nTopic: 8 Word: 0.001*\"navig\" + 0.001*\"channel\" + 0.001*\"gray\" + 0.000*\"index\" + 0.000*\"gray-fuzzi\" + 0.000*\"first-level\" + 0.000*\"weight\" + 0.000*\"evalu\" + 0.000*\"set-valu\" + 0.000*\"safeti\"\nTopic: 9 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 10 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 11 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 12 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 13 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 14 Word: 0.001*\"busi\" + 0.001*\"combat\" + 0.000*\"competit\" + 0.000*\"bart\" + 0.000*\"clausewitz\" + 0.000*\"philosophi\" + 0.000*\"isbn\" + 0.000*\"victori\" + 0.000*\"market\" + 0.000*\"territori\"\nTopic: 15 Word: 0.000*\"chandranigahpur\" + 0.000*\"chan1\" + 0.000*\"characteris-\" + 0.000*\"chao-yang\" + 0.000*\"chantal\" + 0.000*\"chang1\" + 0.000*\"charité\" + 0.000*\"chalder\" + 0.000*\"chan-\" + 0.000*\"charli\"\nTopic: 16 Word: 0.001*\"app\" + 0.001*\"manifesto\" + 0.001*\"religion\" + 0.000*\"usabl\" + 0.000*\"mhealth\" + 0.000*\"jmir\" + 0.000*\"uhealth\" + 0.000*\"usda\" + 0.000*\"food\" + 0.000*\"religi\"\nTopic: 17 Word: 0.001*\"liouvill\" + 0.001*\"fusion\" + 0.000*\"sector\" + 0.000*\"super\" + 0.000*\"sarkissian\" + 0.000*\"poghosyan\" + 0.000*\"matrix\" + 0.000*\"conform\" + 0.000*\"458–479\" + 0.000*\"defect\"\nTopic: 18 Word: 0.001*\"agent\" + 0.001*\"biolog\" + 0.001*\"pest\" + 0.000*\"success\" + 0.000*\"parasitoid\" + 0.000*\"عم��ا\" + 0.000*\"insect\" + 0.000*\"target\" + 0.000*\"لوسر\" + 0.000*\"jurist\"\nTopic: 19 Word: 0.001*\"religion\" + 0.001*\"health\" + 0.000*\"mediat\" + 0.000*\"media\" + 0.000*\"nordic\" + 0.000*\"household\" + 0.000*\"plcds\" + 0.000*\"chronic\" + 0.000*\"hospit\" + 0.000*\"regular\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidfCorpus, num_topics=20, id2word=listOfWords, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore: 0.9146587252616882\t \nTopic: 0.001*\"religion\" + 0.001*\"health\" + 0.000*\"mediat\" + 0.000*\"media\" + 0.000*\"nordic\" + 0.000*\"household\" + 0.000*\"plcds\" + 0.000*\"chronic\" + 0.000*\"hospit\" + 0.000*\"regular\"\n\nScore: 0.048047956079244614\t \nTopic: 0.001*\"land\" + 0.001*\"medicin\" + 0.001*\"catch\" + 0.001*\"trial\" + 0.001*\"complementari\" + 0.001*\"consolid\" + 0.001*\"patient\" + 0.000*\"forc\" + 0.000*\"shadmehr\" + 0.000*\"motor\"\n\nScore: 0.033585503697395325\t \nTopic: 0.001*\"religion\" + 0.001*\"religi\" + 0.000*\"teacher\" + 0.000*\"danger\" + 0.000*\"teach\" + 0.000*\"respond\" + 0.000*\"essenti\" + 0.000*\"educ\" + 0.000*\"question\" + 0.000*\"2018\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpusOfWords[3]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  }
 ]
}